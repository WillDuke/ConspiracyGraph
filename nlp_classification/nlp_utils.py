import json
import pickle
import nltk
import unicodedata
from nltk.cluster.kmeans import KMeansClusterer
import numpy as np
import re
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import KFold
from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Doc2Vec
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from scipy.sparse import save_npz
from tqdm import tqdm

ORIGINAL_RESULTS = '../data/conspiracy_results.json'
COMMENTS = '../data/comments.json'
VALID_IDS = '../data/valid_ids.txt'
# SAVE_PATH = '../data/preproc_data.pkl'

class ConspiracyDataPreparer():
    """Class that combines the data from the original work, the comments
    extracted from the Google API, and the valid IDs and saves a list
    that is compatible with the pipeline."""
    
    def __init__(
        self, debug = False,
        video_info_path = ORIGINAL_RESULTS,
        comments_path = COMMENTS,
        valid_ids_path = VALID_IDS
    ) -> None:

        self.videoinfo_path = video_info_path
        self.comments_path = comments_path
        self.valid_ids_path = valid_ids_path

        self.debug = debug
        self.data = None

    def load_data(self):
        with open(self.videoinfo_path, 'rb') as f:
            video_info = json.load(f)

        with open(self.comments_path, 'rb') as f:
            comments = json.load(f)

        with open(self.valid_ids_path, 'r') as f:
            valid_ids = f.readlines()[0].split(', ')

        return valid_ids, video_info, comments
    
    def tag_parser(self, tags):
        """Handle the nonstandard tags."""

        if tags is None:
            return []

        elif type(tags) is list:
            return tags

        elif type(tags) is str:
            return tags.split('|')
        
    def combine(self):

        valid_ids, video_info, comments = self.load_data()

        if self.debug:
            valid_ids = valid_ids[:100]

        combined = []
        for vid in valid_ids:
            tags = video_info['tags'][vid]
            combined.append({
                'id': vid,
                'title': video_info['title'][vid],
                'description': video_info['description'][vid],
                'tags': " ".join(self.tag_parser(tags)),
                'comments': " ".join([info['text'] for info in comments[vid]])
            })

        self.data = combined

    def normalize_comments(self):

        normalizer = TextNormalizer()
        if self.data:
            for dct in tqdm(self.data):
                dct['comments'] = list(normalizer.transform([dct['comments']]))[0]
        else:
            print('You must run combine() before normalizing the comments.')
    
    def save(self, path):

        with open(path, 'wb') as f:
            pickle.dump(self.data, f)

        print(f'Saved {len(self.data)} items to {path}.')

class ConspiracyLoader():
    """Loader for cross validation, loads the file generated by
    ConspiracyDataPreparer and the valid_scores file. Discretizes the
    scores and implements custom iteration to ease CV with the pipe."""
    
    def __init__(self, folds = 5, shuffle = True, data = None) -> None:
        
        # load list of dictionaries
        if data:
            self.data = data
        else:
            with open('../data/training_preproc_data.pkl', 'rb') as f:
                self.data = pickle.load(f)
        
        # scores = np.load("../data/valid_scores.npy")
        # self.labels = self.discretize_scores(scores)

        with open('../data/training_set.json', 'rb') as f:
            training_set = json.load(f)

        with open('../data/training_valid_ids.txt', 'r') as f:
            valid_ids = f.readlines()[0].split(', ')
        
        self.labels = [training_set['label'].get(id) for id in valid_ids]
        self.labels = np.array([1 if x == 1 else 0 for x in self.labels])
        self.folds = KFold(n_splits = folds, shuffle = shuffle)
    
    def discretize_scores(self, scores):
        """Noah's function for grouping the conspiracy scores."""
        def score_classes(score) : 
            if float(score) < 0.65 : 
                return 0
            elif float(score) < 0.80 : 
                return 1
            elif float(score) < 0.90 : 
                return 2
            else : 
                return 3
        
        return np.asarray([score_classes(s) for s in scores])
    
    def __iter__(self):

        for train_index, test_index in self.folds.split(self.data):
            X_train = [self.data[i] for i in train_index]
            y_train = self.labels[train_index]

            X_test = [self.data[i] for i in test_index]
            y_test = self.labels[test_index]

            yield X_train, X_test, y_train, y_test

class EntityExtractor(BaseEstimator, TransformerMixin):

    def __init__(self, part) -> None:
        super().__init__()

        self.part = part
    
    def fit(self, X, y = None):
        return self
    
    def transform(self, dataloader):
        
        for data in dataloader:
            yield data[self.part]

class TextNormalizer(BaseEstimator, TransformerMixin):

    def __init__(self, language = 'english', passthrough = False) -> None:
        self.stopwords = set(nltk.corpus.stopwords.words(language))
        self.stopwords.remove('they') # remove word important for conspiracies
        self.lemmatizer = nltk.WordNetLemmatizer()
        self.passthrough = passthrough
    
    def is_punct(self, token):
        return(all(unicodedata.category(char).startswith('P') for char in token))

    def is_stopword(self, token):
        return token.lower() in self.stopwords
    
    def lemmatize(self, token, pos_tag):
        tag = {'N': wn.NOUN,
               'V': wn.VERB,
               'R': wn.ADV,
               'J': wn.ADJ
            }.get(pos_tag[0], wn.NOUN)

        return self.lemmatizer.lemmatize(token, tag)

    def remove_punctuation(words):
        """Remove punctuation from list of tokenized words"""
        new_words = []
        for word in words:
            new_word = re.sub(r'[^\w\s]', '', word)
            if new_word != '':
                new_words.append(new_word)
        return new_words

    def replace_numbers(words):
        """Replace all interger occurrences in list of tokenized words with textual representation"""
        p = inflect.engine()
        new_words = []
        for word in words:
            if word.isdigit():
                new_word = p.number_to_words(word)
                new_words.append(new_word)
            else:
                new_words.append(word)
        return new_words
    
    def pre_clean(self, text):

        # remove newlines
        text = re.sub(r'\n', ' ', text)
        # normalize spaces
        text = re.sub(' +', ' ', text)
        # remove urls
        return re.sub(r'http\S+', '', text)

    def normalize(self, text):
        return [self.lemmatize(token, tag).lower()
                for (token, tag) in nltk.pos_tag(nltk.word_tokenize(text))
                if not self.is_punct(token) and not self.is_stopword(token) and len(token) > 2]
    
    def fit(self, X, y = None):
        return self
         
    def transform(self, text_list):
        if not self.passthrough:
            for comments in text_list:
                yield self.normalize(comments)
        else:
            for comments in text_list:
                yield comments


class Doc2VecModel():
    def fit(self, X, y = None):
        return self
    
    def transform(self, comments):

        corpus = [
            TaggedDocument(words, [f'd{idx}'])
            for idx, words in enumerate(comments)
        ]

        model = Doc2Vec(vector_size=300, window=10, min_count=3, 
                    workers=4, epochs = 40)
        model.build_vocab(corpus)
        model.train(corpus, 
                    total_examples=model.corpus_count, 
                    epochs=model.epochs)

        return np.array(model.docvecs.doctag_syn0)

def create_pipeline(estimator = None, lsa = False, prenormalized = False):
    """Create a pipeline to clean, tokenize, lemmatize, and vectorize
    the titles, descriptions, tags, and comments for the video dataset."""

    model = Pipeline([
        ('text_union', FeatureUnion(
            transformer_list = [
                ('comments', Pipeline([
                    ('comments_extractor', EntityExtractor('comments')),
                    ('normalize', TextNormalizer(passthrough=prenormalized)),
                    ('Doc2vec', Doc2VecModel())
                ])),
                ('descriptions', Pipeline([
                    ('descript_extractor', EntityExtractor('description')),
                    ('desc_vect', TfidfVectorizer(
                        ngram_range = (1,2),
                        stop_words=stopwords.words('english')
                    ))
                ])),
                ('tags', Pipeline([
                    ('tags_extractor', EntityExtractor('tags')), 
                    ('tags_vect', TfidfVectorizer(
                        stop_words=stopwords.words('english')
                    ))
                ])),
                ('titles', Pipeline([
                    ('titles_extractor', EntityExtractor('title')),
                    ('titles_vect', TfidfVectorizer(
                        ngram_range = (1,2),
                        stop_words=stopwords.words('english')
                    ))
                ]))
            ],
            transformer_weights = {
                'comments': 0.3,
                'descriptions': 0.3,
                'tags': 0.2,
                'titles': 0.2
            }
        ))
    ])

    # add dimension reduction if requested
    if lsa:
        model.steps.append(['lsa', TruncatedSVD(n_components = 10000)])

    if estimator:
        # add the classifier at the end of the model
        model.steps.append(('classifier', estimator))
        
    return model

def create_adj_matrix(save = True):
    """Create the full adjacency matrix for use elsewhere."""

    preparer = ConspiracyDataPreparer().combine()
    data = preparer.data
    pipeline = create_pipeline()
    combined_matrix = pipeline.fit_transform(data)
    
    if save:
        save_npz('../data/combined_adj_matrix.npz', combined_matrix)

    return combined_matrix

class KMeansClusters(BaseEstimator, TransformerMixin):

    def __init__(self, k = 7) -> None:
        
        self.k = k
        self.distance = nltk.cluster.cosine_distance
        self.model = KMeansClusterer(
            self.k, self.distance, avoid_empty_clusters = True
        )
    
    def fit(self, data, labels = None):
        return self
    
    def transform(self, data):
        return self.model.cluster(data, assign_clusters=True)