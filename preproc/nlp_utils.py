import json
import pickle
import nltk
import unicodedata
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.model_selection import KFold
from gensim.models.doc2vec import Doc2Vec, TaggedDocument, Doc2Vec
from nltk.corpus import wordnet as wn
from nltk.corpus import stopwords
from scipy.sparse import save_npz
from tqdm import tqdm

class ConspiracyDataPreparer():
    """Class that combines the data from the original work, the comments
    extracted from the Google API, and the valid IDs and saves a list
    that is compatible with the pipeline."""
    
    def __init__(self, debug = False) -> None:
        self.videoinfo_path = '../data/conspiracy_results.json'
        self.comments_path = '../data/comments.json'
        self.valid_ids_path = '../data/valid_ids.txt'

        self.debug = debug
        self.data = None

    def load_data(self):
        with open(self.videoinfo_path, 'rb') as f:
            video_info = json.load(f)

        with open(self.comments_path, 'rb') as f:
            comments = json.load(f)

        with open(self.valid_ids_path, 'r') as f:
            valid_ids = f.readlines()[0].split(', ')

        return valid_ids, video_info, comments
    
    def tag_parser(self, tags):
        """Handle the nonstandard tags."""

        if tags is None:
            return []

        elif type(tags) is list:
            return tags

        elif type(tags) is str:
            return tags.split('|')
        
    def combine(self):

        valid_ids, video_info, comments = self.load_data()

        if self.debug:
            valid_ids = valid_ids[:50]

        combined = []
        for vid in valid_ids:
            tags = video_info['tags'][vid]
            combined.append({
                'id': vid,
                'title': video_info['title'][vid],
                'description': video_info['description'][vid],
                'tags': self.tag_parser(tags),
                'comments': [info['text'] for info in comments[vid]]
            })

        self.data = combined
    
    def save(self, path = '../data/preproc_data.pkl'):

        with open(path, 'wb') as f:
            pickle.dump(self.data, f)

        print(f'Saved {len(self.data)} items to {path}.')

class ConspiracyLoader():
    """Loader for cross validation, loads the file generated by
    ConspiracyDataPreparer and the valid_scores file. Discretizes the
    scores and implements custom iteration to ease CV with the pipe."""
    
    def __init__(self, folds = 5, shuffle = True) -> None:
        
        # load list of dictionaries
        with open('../data/preproc_data.pkl', 'rb') as f:
            self.data = pickle.load(f)
        
        scores = np.load("../data/valid_scores.npy")
        self.labels = self.discretize_scores(scores)
        self.folds = KFold(n_splits = folds, shuffle = shuffle)
    
    def discretize_scores(self, scores):
        """Noah's function for grouping the conspiracy scores."""
        def score_classes(score) : 
            if float(score) < 0.65 : 
                return 0
            elif float(score) < 0.80 : 
                return 1
            elif float(score) < 0.90 : 
                return 2
            else : 
                return 3
        
        return np.asarray([score_classes(s) for s in scores])
    
    def __iter__(self):

        for train_index, test_index in self.folds.split(self.data):
            X_train = [self.data[i] for i in train_index]
            y_train = self.labels[train_index]

            X_test = [self.data[i] for i in test_index]
            y_test = self.labels[test_index]

            yield X_train, X_test, y_train, y_test

class TitlesExtractor(BaseEstimator, TransformerMixin):
    def fit(self, X, y = None):
        return self
    
    def transform(self, dataloader):
        for data in dataloader:
            yield data['title']
            
class DescriptionExtractor(BaseEstimator, TransformerMixin):
    def fit(self, X, y = None):
        return self

    def transform(self, dataloader):
        for data in dataloader:
            yield data['description']

class TagsExtractor(BaseEstimator, TransformerMixin):
    def fit(self, X, y = None):
        return self

    def transform(self, dataloader):
        for data in dataloader:
            yield " ".join(data['tags'])

class CommentsExtractor(BaseEstimator, TransformerMixin):
    def fit(self, X, y = None):
        return self
    
    def transform(self, dataloader):
        for data in dataloader:
            yield data['comments']

class TextNormalizer(BaseEstimator, TransformerMixin):

    def __init__(self, language = 'english') -> None:
        self.stopwords = set(nltk.corpus.stopwords.words(language))
        self.lemmatizer = nltk.WordNetLemmatizer()
    
    def is_punct(self, token):
        return(all(unicodedata.category(char).startswith('P') for char in token))

    def is_stopword(self, token):
        return token.lower() in self.stopwords
    
    def lemmatize(self, token, pos_tag):
        tag = {'N': wn.NOUN,
               'V': wn.VERB,
               'R': wn.ADV,
               'J': wn.ADJ
            }.get(pos_tag[0], wn.NOUN)

        return self.lemmatizer.lemmatize(token, tag)

    def normalize(self, text):
        return [self.lemmatize(token, tag).lower()
                for (token, tag) in nltk.pos_tag(nltk.word_tokenize(" ".join(text)))
                if not self.is_punct(token) and not self.is_stopword(token)]
    
    def fit(self, X, y = None):
        return self
         
    def transform(self, text_list):

        for comments in tqdm(text_list):
            yield self.normalize(comments)

class Doc2VecModel():
    def fit(self, X, y = None):
        return self
    
    def transform(self, comments):

        corpus = [
            TaggedDocument(words, [f'd{idx}'])
            for idx, words in enumerate(comments)
        ]

        model = Doc2Vec(vector_size=300, window=10, min_count=3, 
                    workers=4, epochs = 40)
        model.build_vocab(corpus)
        model.train(corpus, 
                    total_examples=model.corpus_count, 
                    epochs=model.epochs)

        print('Doc2Vec vector conversion complete.')
        return np.array(model.docvecs.doctag_syn0)

def create_pipeline(estimator = None, lsa = False):
    """Create a pipeline to clean, tokenize, lemmatize, and vectorize
    the titles, descriptions, tags, and comments for the video dataset."""

    model = Pipeline([
        ('text_union', FeatureUnion(
            transformer_list = [
                ('comments', Pipeline([
                    ('comments_extractor', CommentsExtractor()),
                    ('normalize', TextNormalizer()),
                    ('Doc2vec', Doc2VecModel())
                ])),
                ('descriptions', Pipeline([
                    ('descript_extractor', DescriptionExtractor()),
                    ('desc_vect', TfidfVectorizer(
                        stop_words=stopwords.words('english')
                    ))
                ])),
                ('tags', Pipeline([
                    ('tags_extractor', TagsExtractor()), 
                    ('tags_vect', TfidfVectorizer(
                        stop_words=stopwords.words('english')
                    ))
                ])),
                ('titles', Pipeline([
                    ('titles_extractor', TitlesExtractor()),
                    ('titles_vect', TfidfVectorizer(
                        stop_words=stopwords.words('english')
                    ))
                ]))
            ],
            transformer_weights = {
                'comments': 0.3,
                'descriptions': 0.3,
                'tags': 0.2,
                'titles': 0.2
            }
        ))
    ])

    # add dimension reduction if requested
    if lsa:
        model.steps.append(['lsa', TruncatedSVD(n_components = 10000)])

    if estimator:
        # add the classifier at the end of the model
        model.steps.append(('classifier', estimator))
        
    return model

def create_adj_matrix(save = True):
    """Create the full adjacency matrix for use elsewhere."""

    data = ConspiracyDataPreparer().combine().data
    pipeline = create_pipeline()
    combined_matrix = pipeline.fit_transform(data)
    
    if save:
        save_npz('../data/combined_adj_matrix.npz', combined_matrix)

    return combined_matrix